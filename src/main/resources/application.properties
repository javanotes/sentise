###########################
### WEKA Configurations ###
###########################

#weka.classifiers.lazy.KStar
#weka.classifiers.lazy.LWL -W weka.classifiers.bayes.NaiveBayes
#weka.classifiers.lazy.IBk
#weka.classifiers.lazy.IB1
## The updateable Weka classifier class to be used. Default weka.classifiers.bayes.NaiveBayesUpdateable
#weka.classifier=weka.classifiers.bayes.NaiveBayesMultinomialUpdateable
weka.classifier=weka.classifiers.lazy.IBk

## The option string that may be passed to the classifier. This is where we would specify algorithm specific parameter settings
#weka.classifier.options=-W weka.classifiers.functions.SMO

## Comma separated value of different domain names for which classifier build can be done. This is done in order to build multiple classifiers
## based on different data definitions. Default, a single model named $DEF.
#weka.classifier.domains=MOVIES

## DEPRECATED
## Whether to filter the training string using an analyzer. Default true. This option will be ignored in case of vectorised request data.
#weka.classifier.tokenize=false

## DEPRECATED
## Option string for Weka StringToWordVector tokenizer.
#weka.classifier.tokenize.options=

## Whether to use Lucene as the text analyzer. Default false.
weka.classifier.tokenize.useLucene=true

## The path to the trigger directory for training dataset. Dataset can be loaded using Weka TextDirectoryLoader convention. The trigger file
## should have an extension of '.train' and contain in plain text <domain>:<path_to_class_directories>. 'domain' is optional, and in that case
## will use $DEF
weka.classifier.train.dirPath=C:\\Users\\esutdal\\WORK\\logs\\training

## Number of threads while reading the training dataset concurrently. Default available cores.
#weka.classifier.train.fileio.threads=

## Max time in minutes, to wait for training dataset file read to complete. Default 10.
#weka.classifier.train.fileio.maxAwaitMins=

## Whether to enable submitting the training data in split chunks. True/false, default false. Chunks would be distributed across the cluster to different classifiers.
## So the sample distribution might not be even for a model build. Unless the dataset size is huge, it is advisable not to split the training data. Instead, as a process,
## it can be practiced to sample data at every node level instead.
#weka.classifier.train.dataset.batch=


## Batch size into which the total training dataset will be split. Each split will be submitted for training a classifier across the cluster. Default 100.
#weka.classifier.train.dataset.batch.size=

## The classifier is updated asynchronously in a separate thread, to avoid synchronization. The client thread would submit to a queue.
## This specifies the size of this queue. The queue is not failsafe however. Default 1000.
#weka.classifier.request.backlog=

## The incrementally built classifier is cached locally intermittently. This specify the directory to which the file will be stored. It will be
## stored under ../_supervised/_domains subdirectory. Default system property 'user.dir'
#weka.classifier.cache.path=C:\\Users\\esutdal\\Pictures\\Picasa\\Exports\\exports

## Time interval in seconds, after which the current model snapshot is saved to file. Default 60 secs.
#weka.classifier.cache.sync.intervalSecs=

## The ensemble approach to be followed in combining multiple classifiers into one. VOTING or STACKING. Default VOTING.
#weka.classifier.combiner=

## Ensemble combiner option string
#weka.classifier.combiner.options=

## Max time in seconds, to wait for snapshot models requested from cluster members during an ensemble model build. Default 600.
## If the response is not received in that time, a CombinerResult of TIMEOUT will be returned.
#weka.classifier.combiner.snapAwaitSecs=

## Whether to enable the sentence level sentiment logging, as per Stanford NLP analyzer. Default false.
#snlp.analyzer.enablePrint=


##############################
## Hazelcast Configurations ##
##############################
## For Hazelcast configurations, it is recommended to be done in the standard externalized XML
## by providing a configuration XML path at 'spring.hazelcast.config', 
## and specifying @SpringBootApplication(exclude = {HazelcastAutoConfiguration.class}). This is important to allow proper group joining.
spring.hazelcast.config=hazelcast.xml

## Unique identifier for this running instance in the cluster. Optional
#instance.id=node-1


#########################
## REST Configurations ##
#########################

## To enable REST transport. Default disabled.
rest.enable=true

## HTTP listening port
rest.server.port=8081

## Range offset till which port will be incremented in order to find an available port. Default 100.
#rest.server.port-offset=

## Netty acceptor thread count. Default 2.
#rest.server.ioThreads=


rest.server.mappings.build=org.reactivetechnologies.sentigrade.weka.handlers.BuildRequestHandler
rest.server.mappings.classify=org.reactivetechnologies.sentigrade.weka.handlers.ClassifyRequestHandler

## Server executor thread count. Default 8.
#rest.server.execThreads=

## Context path to be appended after http://<hosturl>/<ctx>/..
rest.server.context-path=/sentigrade


##############################
## Scheduler Configurations ##
##############################

## No of scheduler threads. Default 4
scheduler.threadPoolSize=10

## Await in seconds while terminating scheduler threads. Default 0
scheduler.awaitTerminationSeconds=1

# NOTE: There are some configurations to be provided in AbstractScheduledTask subclasses. 
# Check javadocs for the same.


##############################
## Messaging Configurations ##
##############################

## Whether to clear all pending entries on startup, or just the locally owned entries. Default false (local entries only). This
## may be needed in testing scenario only
#container.clear_all_pending=true

## Whether to remove a consumed entry synchronously from Hazelcast. Default false, to be removed in an asynchronous manner.
#container.remove_entry_immediate=true

## Whether to hold an exclusive lock before start processing an entry. This will introduce somewhat stronger consistency in
## a distributed processing environment, at the cost of an added complexity. What consistency? Say an entry has been completed
## processing in a node, but yet to be committed (removed from distributed map) and this node goes down. The entry primary will be assigned
## to another node on account of partition migration, it will get consumed again. Default false.
#container.process_entry_exclusive=true

## comma separated Path to deployable components for consumer. All jar files under this directory will be added to classpath.
#container.deploy.dir=

## comma separated FQCN of AbstractQueueListener implementation class/s, which are to be registered as Data consumers.
#container.deploy.consumer_class=


############################
## Logger Configurations ###
############################

# Uncomment to disable console logging
#logging.pattern.console=

logging.file=logs/sentigrade.log
logging.pattern.file=%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %msg%n

# Spring
logging.level.org.springframework.boot=WARN
logging.level.io.moquette=WARN
#logging.level.io.netty=DEBUG

# Moquette logging netty handler. This logger to be set to INFO for protocol level logging.
logging.level.messageLogger=WARN

# Application logging
#logging.level.reactivetechnologies.analytics.sentise=DEBUG
#logging.level.org.reactivetechnologies.ticker=DEBUG

# Hazelcast
logging.level.com.hazelcast.internal.cluster=INFO
logging.level.com.hazelcast.cluster=INFO
logging.level.com.hazelcast.system=INFO
logging.level.com.hazelcast=WARN
